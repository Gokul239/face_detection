{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dlib\n",
    "import cv2\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import torch\n",
    "from transformers import ViTModel, ViTFeatureExtractor\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.schema import Document\n",
    "from PIL import Image\n",
    "import warnings\n",
    "import os\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pre-trained ViT model and feature extractor\n",
    "detector = dlib.get_frontal_face_detector()\n",
    "model = ViTModel.from_pretrained('google/vit-base-patch16-224')\n",
    "feature_extractor = ViTFeatureExtractor.from_pretrained('google/vit-base-patch16-224')\n",
    "vector_store = Chroma(persist_directory=\"./chroma_db\", embedding_function=None)\n",
    "known_image_embedding = []\n",
    "known_names = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def face_detector(gray_image, img):\n",
    "    # Detect faces\n",
    "    faces = detector(gray_image)\n",
    "\n",
    "    face_region = []\n",
    "        # Draw rectangles around faces\n",
    "    for i, face in enumerate(faces):\n",
    "            x, y, w, h = (face.left(), face.top(), face.width(), face.height())\n",
    "            face_region.append(img[y:y + h, x:x + w])\n",
    "    return face_region, faces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_embedding(image):\n",
    "        inputs = feature_extractor(images=image, return_tensors='pt')\n",
    "        # Perform inference to get embeddings\n",
    "        with torch.no_grad():\n",
    "                outputs = model(**inputs)\n",
    "                embeddings = outputs.last_hidden_state  # Shape: (batch_size, sequence_length, hidden_size)\n",
    "\n",
    "        # Extract the embedding for the [CLS] token\n",
    "        cls_embedding = embeddings[:, 0, :]  # The first token represents the image embedding\n",
    "\n",
    "        # Convert to numpy array if needed\n",
    "        embedding_array = cls_embedding.numpy()\n",
    "        return embedding_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_images_finc(path, names):\n",
    "    known_image_embedding = []\n",
    "    known_names = []\n",
    "    for i in os.listdir(path):\n",
    "        img = cv2.imread(i)\n",
    "        gray_image = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "        face_region, face_region_box = face_detector(gray_image, img)\n",
    "        if len(face_region) >0:\n",
    "            known_image_embedding_1 = [image_embedding(i) for i in face_region]\n",
    "            known_names_1 = [Document(page_content=names) for i in known_image_embedding_1]\n",
    "            known_image_embedding.extend(known_image_embedding_1)\n",
    "            known_names.extend(known_names_1)\n",
    "            vector_store.add_documents(documents=known_names_1, embeddings=known_image_embedding_1)\n",
    "    return known_image_embedding, known_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={}, page_content='names'),\n",
       " Document(metadata={}, page_content='names'),\n",
       " Document(metadata={}, page_content='names'),\n",
       " Document(metadata={}, page_content='names'),\n",
       " Document(metadata={}, page_content='names'),\n",
       " Document(metadata={}, page_content='names'),\n",
       " Document(metadata={}, page_content='names'),\n",
       " Document(metadata={}, page_content='names'),\n",
       " Document(metadata={}, page_content='names'),\n",
       " Document(metadata={}, page_content='names')]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "known_image_embedding_1 = [Document(embedding = (i), page_content='names') for i in range(10)]\n",
    "known_image_embedding_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed, names = training_images_finc('Training_images/gk')\n",
    "known_image_embedding.extend(embed)\n",
    "known_names.extend(names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "fontScale = 2\n",
    "color = (0, 255, 0)  # Green color in BGR\n",
    "thickness = 10\n",
    "lineType = cv2.LINE_AA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def video_idetification(frame_image):\n",
    "    u_grey = cv2.cvtColor(frame_image, cv2.COLOR_BGR2GRAY)\n",
    "    un_face_region, unknow_region = face_detector(u_grey, frame_image)\n",
    "    un_known_image_embedding = [image_embedding(i) for i in un_face_region]\n",
    "    for j, know_embed in enumerate(known_image_embedding):\n",
    "        for  i, u_embed in enumerate(un_known_image_embedding):\n",
    "            similarity = cosine_similarity(know_embed, u_embed)\n",
    "            print(similarity)\n",
    "            if similarity > 0.2:\n",
    "                x, y, w, h = (unknow_region[i].left(), unknow_region[i].top(), unknow_region[i].width(), unknow_region[i].height())\n",
    "                cv2.rectangle(frame_image, (x, y), (x + w, y + h), (0, 0, 255), 4)\n",
    "                cv2.putText(frame_image, known_names[j], (x, y-60), font, fontScale, color, thickness, lineType)\n",
    "    return frame_image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(unknown_face)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.imwrite('file.jpeg', unknown_face)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Open the default camera (usually the webcam)\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "if not cap.isOpened():\n",
    "    print(\"Error: Could not open camera.\")\n",
    "else:\n",
    "    while True:\n",
    "        # Capture frame-by-frame\n",
    "        ret, frame = cap.read()\n",
    "        frame = video_idetification(frame)\n",
    "        # Display the resulting frame in a popup window\n",
    "        cv2.imshow('Camera Feed', frame)\n",
    "\n",
    "        # Exit on pressing 'q'\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    # Release the camera and close the popup window\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Step 1: Initialize the embedding model (OpenAI in this case, can also be HuggingFace)\n",
    "# You can specify persist_directory to store embeddings on disk (persistent mode) or leave it blank for in-memory mode\n",
    "\n",
    "# Step 3: Prepare the documents for embedding\n",
    "docs = [\n",
    "    Document(page_content=\"The quick brown fox jumps over the lazy dog.\", metadata={\"source\": \"text1\"}),\n",
    "    Document(page_content=\"Artificial Intelligence is transforming industries.\", metadata={\"source\": \"text2\"}),\n",
    "]\n",
    "\n",
    "# Step 4: Add documents to ChromaDB (saves embeddings)\n",
    "vector_store.add_documents(docs)\n",
    "\n",
    "# Step 5: Query ChromaDB using the same embedding model\n",
    "query = \"What jumps over the lazy dog?\"\n",
    "query_embedding = embeddings.embed_query(query)\n",
    "\n",
    "# Step 6: Perform similarity search in ChromaDB\n",
    "results = vector_store.similarity_search(query, k=2)\n",
    "\n",
    "# Display results\n",
    "for result in results:\n",
    "    print(result.page_content, result.metadata)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
